# -*- coding: utf-8 -*-
"""timeseries_toolkit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14SFPj7au3N5rcDZQzOTUJiiKCdJBDASa
"""

"""
Timeseries Toolkit
------------------
ARIMA and GARCH with a scikit-learn–style API + walk-forward CV.
No heavy black-box libraries for the core models; optional statsmodels/arch comparison.

Author: Conner O. Farrell
License: MIT

README
===============================
# Time-Series Forecasting Toolkit (ARIMA/GARCH)

Scikit-learn–style ARIMA and GARCH implemented from scratch with NumPy/pandas and SciPy.
Includes walk-forward cross-validation for realistic OOS testing.

## Install
pip install numpy pandas scipy

## Quickstart
from timeseries_toolkit import ARIMA, GARCH, WalkForwardCV
# see docstrings in timeseries_toolkit.py for runnable examples

## Why this repo?
- Connects econometrics coursework to quant research prep
- Walk-forward CV for proper time-series validation
- Optional comparisons to statsmodels/arch for sanity checks
"""

from __future__ import annotations

import warnings
import logging
import time
from dataclasses import dataclass
from typing import Any, Dict, Optional

import numpy as np
import pandas as pd
from scipy import optimize, stats

logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(message)s")
logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------
# Metric helpers
# ---------------------------------------------------------------------

def _dropna_pair(a: np.ndarray, b: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    mask = ~(np.isnan(a) | np.isnan(b))
    return a[mask], b[mask]


def mse(y_true, y_pred) -> float:
    """Mean squared error.

    Args:
        y_true: array-like of true values
        y_pred: array-like of predictions
    Returns:
        float: MSE
    """
    yt, yp = np.asarray(y_true, float), np.asarray(y_pred, float)
    yt, yp = _dropna_pair(yt, yp)
    return float(np.mean((yt - yp) ** 2))


def rmse(y_true, y_pred) -> float:
    """Root mean squared error.
    """
    return float(np.sqrt(mse(y_true, y_pred)))


def mae(y_true, y_pred) -> float:
    """Mean absolute error."""
    yt, yp = np.asarray(y_true, float), np.asarray(y_pred, float)
    yt, yp = _dropna_pair(yt, yp)
    return float(np.mean(np.abs(yt - yp)))


def mape(y_true, y_pred, eps: float = 1e-8) -> float:
    """Mean absolute percentage error with epsilon guard."""
    yt, yp = np.asarray(y_true, float), np.asarray(y_pred, float)
    yt, yp = _dropna_pair(yt, yp)
    denom = np.maximum(np.abs(yt), eps)
    return float(np.mean(np.abs((yt - yp) / denom)))


_METRICS = {"mse": mse, "rmse": rmse, "mae": mae, "mape": mape}

# ---------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------

def _to_series(y) -> pd.Series:
    """Coerce 1D input to pandas Series with integer index.

    Args:
        y: array-like or Series
    Returns:
        pd.Series
    """
    if isinstance(y, pd.Series):
        return y.reset_index(drop=True)
    y = np.asarray(y).reshape(-1)
    return pd.Series(y)


def _validate_hyperparams(p: int, d: int, q: int):
    if any(v < 0 for v in (p, d, q)):
        raise ValueError("p,d,q must be >= 0")


def _difference(y: pd.Series, d: int) -> pd.Series:
    if d == 0:
        return y.copy()
    yd = y.copy()
    for _ in range(d):
        yd = yd.diff().dropna()
    yd.index = range(len(yd))
    return yd


def _undifference(y_last_values: np.ndarray, y_diff_forecast: np.ndarray, d: int) -> np.ndarray:
    """Reconstruct original scale from differenced forecasts.

    Args:
        y_last_values: last d values of original series (most recent last)
        y_diff_forecast: forecast of differenced series (length h)
        d: integration order
    Returns:
        np.ndarray of length h on original scale
    """
    if d == 0:
        return y_diff_forecast.copy()
    y_out = []
    prev = y_last_values[-1]
    for dy in y_diff_forecast:
        prev = prev + dy
        y_out.append(prev)
    return np.asarray(y_out)


def _lag_matrix(x: np.ndarray, max_lag: int) -> np.ndarray:
    """Construct a row-aligned lag matrix for AR components (internal convenience)."""
    n = len(x)
    if max_lag == 0:
        return np.zeros((n, 0))
    X = np.zeros((n, max_lag))
    for k in range(1, max_lag + 1):
        X[k:, k - 1] = x[:-k]
    return X


def _confint_gaussian(point_pred: np.ndarray, sigma2: float, alpha: float) -> tuple[np.ndarray, np.ndarray]:
    z = stats.norm.ppf(1 - alpha / 2.0)
    half = z * np.sqrt(np.maximum(sigma2, 1e-12))
    lower = point_pred - half
    upper = point_pred + half
    return lower, upper


# ---------------------------------------------------------------------
# ARIMA implementation
# ---------------------------------------------------------------------
class ARIMA:
    """ARIMA(p,d,q) implemented from scratch with a sklearn-style API.

    Notes:
        - Fits an ARMA(p,q) on the d-times differenced series using Gaussian likelihood.
        - MA side uses a simple innovations recursion. For stability we start residuals at 0.
        - Forecasts are produced on the differenced scale then undifferenced back.

    Example:
        >>> import numpy as np
        >>> np.random.seed(0)
        >>> n = 300
        >>> eps = np.random.normal(size=n)
        >>> y = np.zeros(n)
        >>> phi, theta = 0.6, -0.4
        >>> for t in range(1, n):
        ...     dy = (phi * (y[t-1] - (y[t-2] if t-2>=0 else 0))) + eps[t] + theta * (eps[t-1] if t-1>=0 else 0)
        ...     y[t] = y[t-1] + dy
        >>> m = ARIMA(1,1,1).fit(y)
        >>> fc = m.predict(steps=5)
        >>> isinstance(fc, pd.DataFrame)
        True
    """

    def __init__(
        self,
        p: int,
        d: int,
        q: int,
        enforce_stationarity: bool = True,
        enforce_invertibility: bool = True,
        trend: Optional[str] = None,
        optimizer: str = "L-BFGS-B",
        maxiter: int = 200,
        tol: float = 1e-6,
        random_state: Optional[int] = 42,
    ) -> None:
        _validate_hyperparams(p, d, q)
        self.p, self.d, self.q = p, d, q
        self.enforce_stationarity = enforce_stationarity
        self.enforce_invertibility = enforce_invertibility
        self.trend = trend
        self.optimizer = optimizer
        self.maxiter = maxiter
        self.tol = tol
        self.random_state = random_state

        # Learned attributes
        self.params_: Optional[np.ndarray] = None
        self.sigma2_: Optional[float] = None
        self.resid_: Optional[np.ndarray] = None
        self.fitted_: Optional[np.ndarray] = None
        self.y_: Optional[pd.Series] = None
        self.y_d_: Optional[pd.Series] = None

    # ------------------------------ internal likelihood helpers
    @staticmethod
    def _innovations_arima(y_d: np.ndarray, ar: np.ndarray, ma: np.ndarray, const: float) -> tuple[np.ndarray, np.ndarray]:
        """Compute one-step-ahead predictions and residuals recursively.

        Returns (fitted, resid) on differenced scale.
        """
        n = len(y_d)
        p, q = len(ar), len(ma)
        fitted = np.zeros(n)
        resid = np.zeros(n)
        for t in range(n):
            ar_part = 0.0
            for i in range(1, p + 1):
                if t - i >= 0:
                    ar_part += ar[i - 1] * y_d[t - i]
            ma_part = 0.0
            for j in range(1, q + 1):
                if t - j >= 0:
                    ma_part += ma[j - 1] * resid[t - j]
            fitted[t] = const + ar_part + ma_part
            resid[t] = y_d[t] - fitted[t]
        return fitted, resid

    def _neg_loglike(self, theta: np.ndarray, y_d: np.ndarray) -> float:
        # Parameter unpack
        idx = 0
        const = 0.0
        if self.trend == "c" and self.d == 0:
            const = theta[idx]
            idx += 1
        ar = theta[idx : idx + self.p]
        idx += self.p
        ma = theta[idx : idx + self.q]
        idx += self.q
        sigma2 = float(np.maximum(theta[idx], 1e-8))

        # Stationarity/invertibility (soft penalties)
        penalty = 0.0
        if self.enforce_stationarity and self.p > 0:
            # AR polynomial roots outside unit circle
            poly = np.r_[1.0, -ar]
            roots = np.roots(poly) if self.p > 0 else np.array([2.0])
            if np.any(np.abs(roots) <= 1.0):
                penalty += 1e6
        if self.enforce_invertibility and self.q > 0:
            poly = np.r_[1.0, ma]
            roots = np.roots(poly) if self.q > 0 else np.array([2.0])
            if np.any(np.abs(roots) <= 1.0):
                penalty += 1e6

        fitted, resid = self._innovations_arima(y_d, ar, ma, const)
        ll = -0.5 * np.sum(np.log(2 * np.pi * sigma2) + (resid ** 2) / sigma2)
        return float(-(ll) + penalty)

    # ------------------------------ public API
    def fit(self, y: pd.Series | np.ndarray) -> "ARIMA":
        """Fit the ARIMA model.

        Args:
            y: 1D array-like or Series of levels
        Returns:
            self
        Notes:
            Fits ARMA(p,q) on differenced series (d times). Trend only allowed when d==0.
        """
        y = _to_series(y)
        self.y_ = y
        y_d = _difference(y, self.d)
        self.y_d_ = y_d
        if len(y_d) <= max(self.p, self.q) + 1:
            raise ValueError("Series too short for specified (p,d,q)")

        # Initialization
        np.random.seed(self.random_state or 0)
        theta0 = []
        if self.trend == "c" and self.d == 0:
            theta0.append(float(y_d.mean()))
        if self.p > 0:
            theta0 += [0.1 / max(self.p, 1)] * self.p
        if self.q > 0:
            theta0 += [0.1 / max(self.q, 1)] * self.q
        theta0 += [float(np.var(y_d, ddof=1))]
        theta0 = np.asarray(theta0, dtype=float)

        bounds = []
        if self.trend == "c" and self.d == 0:
            bounds.append((None, None))
        bounds += [(-0.999, 0.999)] * self.p  # AR
        bounds += [(-0.999, 0.999)] * self.q  # MA
        bounds += [(1e-8, None)]  # sigma2

        res = optimize.minimize(
            fun=self._neg_loglike,
            x0=theta0,
            args=(y_d.values,),
            method=self.optimizer,
            bounds=bounds if self.optimizer in {"L-BFGS-B", "TNC"} else None,
            options={"maxiter": self.maxiter, "ftol": self.tol},
        )
        if not res.success:
            warnings.warn(f"Optimization did not converge: {res.message}")
        self.params_ = res.x.copy()

        # Store fitted/residuals
        idx = 0
        const = 0.0
        if self.trend == "c" and self.d == 0:
            const = self.params_[idx]
            idx += 1
        ar = self.params_[idx : idx + self.p]
        idx += self.p
        ma = self.params_[idx : idx + self.q]
        idx += self.q
        self.sigma2_ = float(self.params_[idx])
        fitted, resid = self._innovations_arima(y_d.values, ar, ma, const)
        self.fitted_ = fitted
        self.resid_ = resid
        return self

    def predict_in_sample(self, start: int | None = None, end: int | None = None) -> pd.Series:
        """In-sample one-step predictions on the differenced scale, undifferenced back.

        Args:
            start: starting index (inclusive) on original series to report predictions
            end: end index (inclusive)
        Returns:
            pd.Series of predictions on original scale
        Notes:
            The first d observations cannot be predicted due to differencing.
        """
        if self.y_ is None or self.y_d_ is None or self.fitted_ is None:
            raise RuntimeError("Model is not fitted.")
        start = 0 if start is None else start
        end = len(self.y_) - 1 if end is None else end
        # Align differenced fitted to original index by padding d NaNs
        pad = np.full(self.d, np.nan)
        yhat_d = np.r_[pad, self.fitted_]
        # Convert differenced predictions to original via cumulative sum over residuals approx
        # Here we approximate by using y[t-1] + yhat_d[t] for t>=d
        yhat = self.y_.values.copy().astype(float)
        for t in range(self.d, len(self.y_)):
            yhat[t] = yhat[t - 1] + yhat_d[t]
        out = pd.Series(yhat, index=self.y_.index)
        return out.iloc[start : end + 1]

    def predict(self, steps: int, return_conf_int: bool = True, alpha: float = 0.05) -> pd.DataFrame:
        """h-step ahead forecast on original scale.

        Args:
            steps: forecast horizon
            return_conf_int: whether to include Gaussian CIs
            alpha: significance level for CIs
        Returns:
            pd.DataFrame with columns ['mean', 'lower', 'upper'] (lower/upper if requested)
        """
        if self.y_ is None or self.y_d_ is None or self.params_ is None or self.sigma2_ is None:
            raise RuntimeError("Model is not fitted.")
        y = self.y_.values
        y_d = self.y_d_.values

        # unpack params
        idx = 0
        const = 0.0
        if self.trend == "c" and self.d == 0:
            const = self.params_[idx]
            idx += 1
        ar = self.params_[idx : idx + self.p]
        idx += self.p
        ma = self.params_[idx : idx + self.q]
        # sigma2 at end

        # Prepare arrays for forecasting on differenced scale
        resid_hist = self.resid_.tolist() if self.resid_ is not None else [0.0] * len(y_d)
        y_d_hist = y_d.tolist()
        fc_d = []
        for h in range(steps):
            t = len(y_d_hist)
            ar_part = 0.0
            for i in range(1, self.p + 1):
                if t - i >= 0:
                    ar_part += ar[i - 1] * y_d_hist[t - i]
            ma_part = 0.0
            for j in range(1, self.q + 1):
                if t - j >= 0:
                    ma_part += ma[j - 1] * resid_hist[t - j]
            pred = const + ar_part + ma_part
            # Mean forecast sets future residuals to zero
            y_d_hist.append(pred)
            resid_hist.append(0.0)
            fc_d.append(pred)

        # Undifference back
        last_vals = y[-self.d :] if self.d > 0 else np.array([])
        fc = _undifference(last_vals, np.asarray(fc_d), self.d)

        idx_future = pd.RangeIndex(start=len(self.y_), stop=len(self.y_) + steps, step=1)
        out = pd.DataFrame({"mean": fc}, index=idx_future)
        if return_conf_int:
            lower, upper = _confint_gaussian(out["mean"].values, self.sigma2_, alpha)
            out["lower"] = lower
            out["upper"] = upper
        return out

    def score(self, metric: str = "mse") -> float:
        """Score in-sample one-step predictions vs actuals (aligned).

        Args:
            metric: one of {mse, rmse, mae, mape}
        Returns:
            float score
        """
        if self.y_d_ is None or self.fitted_ is None:
            raise RuntimeError("Model is not fitted.")
        # Compare on differenced scale for strict one-step alignment
        y_true = self.y_d_.values
        y_pred = self.fitted_
        func = _METRICS.get(metric.lower())
        if func is None:
            raise ValueError(f"Unknown metric: {metric}")
        return func(y_true, y_pred)

    # sklearn-like param API
    def get_params(self, deep: bool = False) -> Dict[str, Any]:
        return {
            "p": self.p,
            "d": self.d,
            "q": self.q,
            "enforce_stationarity": self.enforce_stationarity,
            "enforce_invertibility": self.enforce_invertibility,
            "trend": self.trend,
            "optimizer": self.optimizer,
            "maxiter": self.maxiter,
            "tol": self.tol,
            "random_state": self.random_state,
        }

    def set_params(self, **params) -> "ARIMA":
        for k, v in params.items():
            if not hasattr(self, k):
                raise ValueError(f"Unknown parameter: {k}")
            setattr(self, k, v)
        return self


# ---------------------------------------------------------------------
# GARCH implementation
# ---------------------------------------------------------------------
class GARCH:
    """GARCH(p,q) with Gaussian likelihood and sklearn-style API.

    Variance recursion: sigma_t^2 = omega + sum(alpha_i * eps_{t-i}^2) + sum(beta_j * sigma_{t-j}^2)

    Args:
        p: ARCH order
        q: GARCH order
        mean: 'zero' or 'constant'
        dist: 'normal' (Student-t TODO)
        optimizer: scipy optimizer name
        maxiter: optimizer iterations
        tol: optimizer tolerance
        random_state: seed for simulation
    """

    def __init__(
        self,
        p: int,
        q: int,
        mean: str = "zero",
        dist: str = "normal",
        optimizer: str = "L-BFGS-B",
        maxiter: int = 500,
        tol: float = 1e-6,
        random_state: Optional[int] = 42,
    ) -> None:
        if p < 0 or q < 0:
            raise ValueError("p,q must be >= 0")
        if mean not in {"zero", "constant"}:
            raise ValueError("mean must be 'zero' or 'constant'")
        if dist != "normal":
            warnings.warn("Only Gaussian likelihood implemented; using 'normal'.")
        self.p, self.q = p, q
        self.mean = mean
        self.dist = dist
        self.optimizer = optimizer
        self.maxiter = maxiter
        self.tol = tol
        self.random_state = random_state

        # learned
        self.params_: Optional[np.ndarray] = None
        self.mu_: float = 0.0
        self.omega_: float | None = None
        self.alpha_: Optional[np.ndarray] = None
        self.beta_: Optional[np.ndarray] = None
        self.eps_: Optional[np.ndarray] = None
        self.sigma2_: Optional[np.ndarray] = None
        self.r_: Optional[pd.Series] = None

    def _neg_loglike(self, theta: np.ndarray, r: np.ndarray) -> float:
        idx = 0
        mu = 0.0
        if self.mean == "constant":
            mu = theta[idx]
            idx += 1
        omega = theta[idx]
        idx += 1
        alpha = theta[idx : idx + self.p]
        idx += self.p
        beta = theta[idx : idx + self.q]

        # constraints via soft penalty
        penalty = 0.0
        if omega <= 0 or np.any(alpha < 0) or np.any(beta < 0):
            penalty += 1e6
        if self.p + self.q > 0 and (alpha.sum() + beta.sum()) >= 0.999:
            penalty += 1e6

        n = len(r)
        eps = np.zeros(n)
        sigma2 = np.full(n, np.var(r, ddof=1))
        for t in range(n):
            eps[t] = r[t] - mu
            arch_term = 0.0
            for i in range(1, self.p + 1):
                if t - i >= 0:
                    arch_term += alpha[i - 1] * (eps[t - i] ** 2)
            garch_term = 0.0
            for j in range(1, self.q + 1):
                if t - j >= 0:
                    garch_term += beta[j - 1] * sigma2[t - j]
            sigma2[t] = omega + arch_term + garch_term
            sigma2[t] = max(sigma2[t], 1e-10)
        ll = -0.5 * np.sum(np.log(2 * np.pi) + np.log(sigma2) + (eps ** 2) / sigma2)
        return float(-(ll) + penalty)

    def fit(self, r: pd.Series | np.ndarray) -> "GARCH":
        """Fit GARCH on returns.

        Args:
            r: 1D returns series
        Returns:
            self
        """
        r = _to_series(r)
        self.r_ = r
        n = len(r)
        if n <= max(self.p, self.q) + 2:
            raise ValueError("Series too short for specified (p,q)")

        # init
        np.random.seed(self.random_state or 0)
        theta0 = []
        if self.mean == "constant":
            theta0.append(float(r.mean()))
        theta0.append(float(np.var(r, ddof=1) * 0.1))  # omega small positive
        theta0 += [0.05] * self.p  # alpha
        theta0 += [0.85 / max(self.q, 1)] * self.q  # beta roughly stable
        theta0 = np.asarray(theta0, float)

        bounds = []
        if self.mean == "constant":
            bounds.append((None, None))
        bounds.append((1e-12, None))  # omega
        bounds += [(0.0, 1.0)] * self.p  # alpha
        bounds += [(0.0, 0.999)] * self.q  # beta

        res = optimize.minimize(
            fun=self._neg_loglike,
            x0=theta0,
            args=(r.values,),
            method=self.optimizer,
            bounds=bounds if self.optimizer in {"L-BFGS-B", "TNC"} else None,
            options={"maxiter": self.maxiter, "ftol": self.tol},
        )
        if not res.success:
            warnings.warn(f"Optimization did not converge: {res.message}")
        self.params_ = res.x.copy()

        # unpack and compute eps/sigma2
        idx = 0
        self.mu_ = 0.0
        if self.mean == "constant":
            self.mu_ = float(self.params_[idx]); idx += 1
        self.omega_ = float(self.params_[idx]); idx += 1
        self.alpha_ = self.params_[idx : idx + self.p]; idx += self.p
        self.beta_ = self.params_[idx : idx + self.q]; idx += self.q

        n = len(r)
        eps = np.zeros(n)
        sigma2 = np.full(n, np.var(r, ddof=1))
        for t in range(n):
            eps[t] = r.values[t] - self.mu_
            arch_term = 0.0
            for i in range(1, self.p + 1):
                if t - i >= 0:
                    arch_term += self.alpha_[i - 1] * (eps[t - i] ** 2)
            garch_term = 0.0
            for j in range(1, self.q + 1):
                if t - j >= 0:
                    garch_term += self.beta_[j - 1] * sigma2[t - j]
            sigma2[t] = max(self.omega_ + arch_term + garch_term, 1e-10)
        self.eps_ = eps
        self.sigma2_ = sigma2
        return self

    def predict_vol(self, steps: int) -> pd.Series:
        """Forecast future volatility (std dev) for given steps.

        Args:
            steps: forecast horizon
        Returns:
            pd.Series of volatility forecasts (length steps)
        """
        if self.r_ is None or self.params_ is None or self.sigma2_ is None or self.eps_ is None:
            raise RuntimeError("Model is not fitted.")
        eps_last = self.eps_.copy()
        sigma2_last = self.sigma2_.copy()
        eps_hist = list(eps_last)
        sig_hist = list(sigma2_last)
        out_sigma2 = []
        for _ in range(steps):
            arch_term = 0.0
            for i in range(1, self.p + 1):
                arch_term += (self.alpha_[i - 1] if self.alpha_ is not None else 0.0) * (eps_hist[-i] ** 2)
            garch_term = 0.0
            for j in range(1, self.q + 1):
                garch_term += (self.beta_[j - 1] if self.beta_ is not None else 0.0) * sig_hist[-j]
            sigma2 = float(max((self.omega_ or 0.0) + arch_term + garch_term, 1e-10))
            out_sigma2.append(sigma2)
            sig_hist.append(sigma2)
            eps_hist.append(0.0)  # mean forecast residual is 0
        vol = np.sqrt(np.asarray(out_sigma2))
        idx_future = pd.RangeIndex(start=len(self.r_), stop=len(self.r_) + steps, step=1)
        return pd.Series(vol, index=idx_future, name="volatility")

    def simulate(self, n: int) -> pd.DataFrame:
        """Simulate returns from fitted GARCH.

        Args:
            n: number of steps
        Returns:
            DataFrame with columns ['eps', 'sigma2', 'r']
        Notes:
            Requires a fitted model to use learned params.
        """
        if self.params_ is None:
            raise RuntimeError("Fit the model before simulation.")
        rng = np.random.default_rng(self.random_state)
        eps = np.zeros(n)
        sigma2 = np.zeros(n)
        mu = self.mu_
        omega = float(self.omega_ or 1e-6)
        alpha = self.alpha_ or np.zeros(self.p)
        beta = self.beta_ or np.zeros(self.q)
        # initialize with unconditional variance if stable
        ucvar = omega / max(1.0 - (alpha.sum() + beta.sum()), 1e-6)
        sigma2[0] = ucvar
        eps[0] = np.sqrt(sigma2[0]) * rng.standard_normal()
        for t in range(1, n):
            arch_term = sum(alpha[i] * (eps[t - i - 1] ** 2) for i in range(min(self.p, t)))
            garch_term = sum(beta[j] * (sigma2[t - j - 1]) for j in range(min(self.q, t)))
            sigma2[t] = max(omega + arch_term + garch_term, 1e-10)
            eps[t] = np.sqrt(sigma2[t]) * rng.standard_normal()
        r = mu + eps
        return pd.DataFrame({"eps": eps, "sigma2": sigma2, "r": r})

    def score(self, metric: str = "mse") -> float:
        """Score variance fit by comparing eps^2 to sigma2 (default MSE)."""
        if self.eps_ is None or self.sigma2_ is None:
            raise RuntimeError("Model is not fitted.")
        func = _METRICS.get(metric.lower())
        if func is None:
            raise ValueError(f"Unknown metric: {metric}")
        return func(self.eps_ ** 2, self.sigma2_)

    def get_params(self, deep: bool = False) -> Dict[str, Any]:
        return {
            "p": self.p,
            "q": self.q,
            "mean": self.mean,
            "dist": self.dist,
            "optimizer": self.optimizer,
            "maxiter": self.maxiter,
            "tol": self.tol,
            "random_state": self.random_state,
        }

    def set_params(self, **params) -> "GARCH":
        for k, v in params.items():
            if not hasattr(self, k):
                raise ValueError(f"Unknown parameter: {k}")
            setattr(self, k, v)
        return self


# ---------------------------------------------------------------------
# Walk-forward cross validation
# ---------------------------------------------------------------------
class WalkForwardCV:
    """Expanding-window walk-forward cross-validation for ARIMA/GARCH.

    Example:
        >>> y = np.cumsum(np.random.randn(400))
        >>> res = WalkForwardCV.evaluate(ARIMA(1,1,1), y, initial_window=200, horizon=1, step=5, metric="rmse")
    """

    @staticmethod
    def evaluate(
        model,
        y: pd.Series | np.ndarray,
        initial_window: int,
        horizon: int = 1,
        step: int = 1,
        metric: str = "mse",
        verbose: bool = True,
    ) -> dict:
        """Run walk-forward CV and return per-fold metrics and summary.

        Args:
            model: unfitted ARIMA or GARCH instance
            y: 1D sequence (levels for ARIMA; returns for GARCH)
            initial_window: number of observations for first training split
            horizon: forecast steps
            step: shift per fold
            metric: one of {mse, rmse, mae, mape}
            verbose: print progress
        Returns:
            dict with keys {fold_metrics, mean, std, n_folds, params}
        """
        y = _to_series(y)
        if initial_window < 1 or initial_window >= len(y):
            raise ValueError("initial_window must be in [1, len(y)-1]")
        func = _METRICS.get(metric.lower())
        if func is None:
            raise ValueError(f"Unknown metric: {metric}")

        fold_metrics: list[float] = []
        t0 = time.time()
        n_folds = 0
        for t in range(initial_window, len(y) - horizon + 1, step):
            # fresh clone
            params = model.get_params()
            mdl = model.__class__(**params)
            mdl.fit(y.iloc[:t])
            if isinstance(mdl, ARIMA):
                fc = mdl.predict(steps=horizon, return_conf_int=False)["mean"].values
                y_true = y.iloc[t : t + horizon].values
                val = func(y_true, fc)
            elif isinstance(mdl, GARCH):
                vol = mdl.predict_vol(steps=horizon).values
                # default: compare to realized squared returns
                y_true = (y.iloc[t : t + horizon].values) ** 2
                val = func(y_true, vol ** 2)
            else:
                raise TypeError("Unsupported model type for WalkForwardCV")
            fold_metrics.append(float(val))
            n_folds += 1
            if verbose and (n_folds % 10 == 0):
                logger.info(f"Fold {n_folds} | {metric}={val:.4f}")
        elapsed = time.time() - t0
        return {
            "fold_metrics": fold_metrics,
            "mean": float(np.mean(fold_metrics)) if fold_metrics else np.nan,
            "std": float(np.std(fold_metrics)) if fold_metrics else np.nan,
            "n_folds": n_folds,
            "params": model.get_params(),
            "elapsed_sec": float(elapsed),
        }


# ---------------------------------------------------------------------
# Optional comparisons to reference libraries
# ---------------------------------------------------------------------

def compare_arima_statsmodels(
    y: pd.Series | np.ndarray, p: int, d: int, q: int, steps: int = 5
) -> dict | None:
    """Compare our ARIMA to statsmodels if available.

    Returns dict with keys {'ours', 'sm', 'mse'} or None if statsmodels missing.
    """
    try:
        from statsmodels.tsa.arima.model import ARIMA as SMARIMA  # type: ignore
    except Exception:
        logger.info("statsmodels not installed; skipping comparison.")
        return None
    y = _to_series(y)
    ours = ARIMA(p, d, q).fit(y)
    fc_ours = ours.predict(steps=steps, return_conf_int=False)["mean"].values
    sm = SMARIMA(y, order=(p, d, q)).fit()
    fc_sm = sm.forecast(steps=steps).values
    return {"ours": fc_ours, "sm": fc_sm, "mse": mse(fc_sm, fc_ours)}


def compare_garch_arch(
    y: pd.Series | np.ndarray, p: int, q: int, steps: int = 5
) -> dict | None:
    """Compare our GARCH to arch package if available (vol forecasts)."""
    try:
        from arch import arch_model  # type: ignore
    except Exception:
        logger.info("arch not installed; skipping comparison.")
        return None
    y = _to_series(y)
    ours = GARCH(p, q).fit(y)
    vol_ours = ours.predict_vol(steps=steps).values
    am = arch_model(y, mean="Zero", vol="GARCH", p=p, q=q, dist="normal")
    res = am.fit(disp="off")
    vol_arch = np.sqrt(res.forecast(horizon=steps).variance.values[-1])
    return {"ours": vol_ours, "arch": vol_arch, "mse": mse(vol_arch, vol_ours)}


# ---------------------------------------------------------------------
# Example Usage
# ---------------------------------------------------------------------

# Example of using the ARIMA model
np.random.seed(0)
n = 300
eps = np.random.normal(scale=1.0, size=n)
y = np.zeros(n)
phi, theta = 0.6, -0.4
for t in range(1, n):
    dy = (phi * (y[t-1] - (y[t-2] if t-2>=0 else 0))) + eps[t] + theta * (eps[t-1] if t-1>=0 else 0)
    y[t] = y[t-1] + dy

m = ARIMA(1, 1, 1).fit(y)
fc = m.predict(steps=5)
print("In-sample MSE:", m.score("mse"))
print(fc)


np.random.seed(0)
n = 1000
r = np.zeros(n)
sigma2 = 1.0
for t in range(1, n):
    sigma2 = 0.05 + 0.10*(r[t-1]**2) + 0.85*sigma2
    r[t] = np.sqrt(sigma2) * np.random.randn()

g = GARCH(p=1, q=1, mean="zero").fit(r)
print(g.predict_vol(steps=5))
print("Variance fit MSE:", g.score("mse"))


np.random.seed(0)
y = np.cumsum(np.random.randn(400))  # random walk
wf = WalkForwardCV.evaluate(ARIMA(1,1,1), y, initial_window=200, horizon=1, step=5, metric="rmse")
print(wf)